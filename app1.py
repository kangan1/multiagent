import streamlit as st
import pinecone
from langchain.vectorstores import Pinecone
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# Load from Streamlit secrets
pinecone_api_key = st.secrets["PINECONE_API_KEY"]
pinecone_env = st.secrets["PINECONE_ENVIRONMENT"]
pinecone_index = st.secrets["PINECONE_INDEX_NAME"]
openai_api_key = st.secrets["OPENAI_API_KEY"]

# üîê Initialize Pinecone
pinecone.init(api_key=pinecone_api_key, environment=pinecone_env)

# üîé Connect to your Pinecone index
index = pinecone.Index(pinecone_index)

# üß† Set up LLM and Embeddings
llm = ChatOpenAI(api_key=openai_api_key, model="gpt-3.5-turbo")
embeddings = OpenAIEmbeddings(api_key=openai_api_key)

# Create VectorStore
vectorstore = Pinecone(index, embeddings.embed_query, "text")

# RetrievalQA chain
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())

# Streamlit UI
st.title("Pinecone + OpenAI RAG App")
query = st.text_input("Enter your question:")

if st.button("Submit") and query:
    answer = qa_chain.run(query)
    st.write("### Answer")
    st.write(answer)
